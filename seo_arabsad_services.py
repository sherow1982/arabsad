#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø³ÙƒØ±Ø¨Øª Ù…ØªÙƒØ§Ù…Ù„ Ù„Ù€ SEO ÙˆLocal Business 
Ø±ÙŠØ¨Ùˆ: arabsad-ads
ØªØµØ­ÙŠØ­: Ø­ÙØ¸ ÙƒØ§Ù…Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø¬Ù…ÙŠØ¹ Ø¯ÙˆÙ„ Ø§Ù„Ø®Ù„ÙŠØ¬
"""

import sys
import re
import json
from pathlib import Path
from datetime import datetime

# ================== Ø¨ÙŠØ§Ù†Ø§Øª Ø¯ÙˆÙ„ Ø§Ù„Ø®Ù„ÙŠØ¬ ÙƒØ§Ù…Ù„Ø© ==================

GULF_COUNTRIES = {
    "SA": {
        "name": "Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©",
        "arabic_name": "Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©",
        "lat": 24.7136,
        "lng": 46.6753,
        "cities": [
            ("Ø§Ù„Ø±ÙŠØ§Ø¶", 24.7136, 46.6753),
            ("Ø¬Ø¯Ø©", 21.5485, 39.1721),
            ("Ø§Ù„Ø¯Ù…Ø§Ù…", 26.3989, 50.2048),
            ("Ø§Ù„Ø®Ø¨Ø±", 26.2156, 50.2106),
            ("Ø§Ù„Ù‚Ø·ÙŠÙ", 26.1801, 50.0157),
            ("Ù…ÙƒØ©", 21.4225, 39.8262),
            ("Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©", 24.4647, 39.6074),
            ("Ø§Ù„Ø·Ø§Ø¦Ù", 21.2745, 40.4158),
            ("ØªØ¨ÙˆÙƒ", 28.3852, 36.5627),
            ("Ø£Ø¨Ù‡Ø§", 18.2155, 42.5054),
            ("Ø¬ÙŠØ²Ø§Ù†", 16.8892, 42.5521),
            ("Ù†Ø¬Ø±Ø§Ù†", 17.6927, 44.1860),
            ("Ø­ÙØ± Ø§Ù„Ø¨Ø§Ø·Ù†", 28.4347, 45.3569),
        ]
    },
    "AE": {
        "name": "Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª",
        "arabic_name": "Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø©",
        "lat": 23.4241,
        "lng": 53.8478,
        "cities": [
            ("Ø¯Ø¨ÙŠ", 25.2048, 55.2708),
            ("Ø£Ø¨ÙˆØ¸Ø¨ÙŠ", 24.4539, 54.3773),
            ("Ø§Ù„Ø´Ø§Ø±Ù‚Ø©", 25.3548, 55.3944),
            ("Ø¹Ø¬Ù…Ø§Ù†", 25.3986, 55.4501),
            ("Ø£Ù… Ø§Ù„Ù‚ÙŠÙˆÙŠÙ†", 25.5645, 55.5597),
            ("Ø±Ø£Ø³ Ø§Ù„Ø®ÙŠÙ…Ø©", 25.7482, 55.9754),
            ("Ø§Ù„ÙØ¬ÙŠØ±Ø©", 25.1242, 56.3540),
        ]
    },
    "KW": {
        "name": "Ø§Ù„ÙƒÙˆÙŠØª",
        "arabic_name": "Ø¯ÙˆÙ„Ø© Ø§Ù„ÙƒÙˆÙŠØª",
        "lat": 29.3759,
        "lng": 47.9774,
        "cities": [
            ("Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„ÙƒÙˆÙŠØª", 29.3759, 47.9774),
            ("Ø§Ù„Ø£Ø­Ù…Ø¯ÙŠ", 29.1118, 47.6929),
            ("Ø§Ù„Ø¬Ù‡Ø±Ø§Ø¡", 29.4444, 47.6804),
            ("Ø§Ù„ÙØ±ÙˆØ§Ù†ÙŠØ©", 29.2269, 47.8558),
            ("Ø­ÙˆÙ„ÙŠ", 29.3621, 47.9825),
            ("Ù…Ø¨Ø§Ø±Ùƒ Ø§Ù„ÙƒØ¨ÙŠØ±", 29.0269, 47.7373),
            ("Ø§Ù„Ø¹Ø§ØµÙ…Ø©", 29.3759, 47.9774),
        ]
    },
    "QA": {
        "name": "Ù‚Ø·Ø±",
        "arabic_name": "Ø¯ÙˆÙ„Ø© Ù‚Ø·Ø±",
        "lat": 25.2854,
        "lng": 51.5310,
        "cities": [
            ("Ø§Ù„Ø¯ÙˆØ­Ø©", 25.2854, 51.5310),
            ("Ø§Ù„Ø±ÙŠØ§Ù†", 25.3548, 51.5342),
            ("Ø§Ù„ÙˆÙƒØ±Ø©", 25.1673, 51.6286),
            ("Ø§Ù„Ø®ÙˆØ±", 25.6753, 51.4805),
            ("Ø£Ù… ØµÙ„Ø§Ù„", 25.4167, 51.5000),
            ("Ø§Ù„Ø´Ù…Ø§Ù„", 25.8500, 51.2500),
        ]
    },
    "BH": {
        "name": "Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†",
        "arabic_name": "Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†",
        "lat": 26.0667,
        "lng": 50.5577,
        "cities": [
            ("Ø§Ù„Ù…Ù†Ø§Ù…Ø©", 26.1290, 50.5826),
            ("Ø§Ù„Ù…Ø­Ø±Ù‚", 26.1667, 50.5833),
            ("Ø§Ù„Ø±ÙØ§Ø¹", 26.1333, 50.4167),
            ("Ø§Ù„Ø¬ÙÙŠØ±", 26.1778, 50.4389),
            ("Ø³Ù„Ù…Ø§Ù† Ø¢Ø¨Ø§Ø¯", 26.0833, 50.5000),
        ]
    },
    "OM": {
        "name": "Ø¹Ù…Ø§Ù†",
        "arabic_name": "Ø³Ù„Ø·Ù†Ø© Ø¹Ù…Ø§Ù†",
        "lat": 21.4735,
        "lng": 55.9754,
        "cities": [
            ("Ù…Ø³Ù‚Ø·", 21.4735, 55.9754),
            ("ØµÙ„Ø§Ù„Ø©", 17.0151, 54.0924),
            ("ØµØ­Ø§Ø±", 24.2795, 56.9366),
            ("Ù†Ø²ÙˆÙ‰", 22.9342, 57.5364),
            ("Ø§Ù„Ø³ÙˆÙŠÙ‚", 23.8069, 57.4074),
            ("Ø´Ù†Ø§Øµ", 24.7167, 56.7833),
            ("Ù‡ÙŠÙ…Ø§Ø¡", 24.2000, 56.6000),
        ]
    },
}

# ================== Ø§Ù„Ø¯ÙˆØ§Ù„ ==================

def extract_title(html: str) -> str:
    m = re.search(r'<title[^>]*>([^<]+)</title>', html, re.IGNORECASE)
    if m:
        txt = m.group(1).strip()
        return txt.split('|')[0].strip() if '|' in txt else txt
    m = re.search(r'<h1[^>]*>([^<]+)</h1>', html, re.IGNORECASE)
    return m.group(1).strip() if m else "ØµÙØ­Ø© Ù…Ù† Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨"

def extract_description(html: str) -> str:
    m = re.search(r'<meta\s+name=["\']description["\']\s+content=["\']([^"\']+)["\']', html, re.IGNORECASE)
    if m:
        return m.group(1).strip()
    m = re.search(r'<p[^>]*>([^<]+)</p>', html, re.IGNORECASE)
    if m:
        txt = m.group(1).strip()
        return txt if len(txt) <= 155 else txt[:152] + "..."
    return "Ø®Ø¯Ù…Ø§Øª ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ù…ØªÙ…ÙŠØ²Ø© Ù…Ù† Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨"

def extract_image(html: str) -> str:
    m = re.search(r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>', html, re.IGNORECASE)
    if m:
        src = m.group(1).strip()
        if src.startswith('http'):
            return src
        src = src.lstrip('./')
        return f"https://sherow1982.github.io/arabsad-ads/{src}"
    return "https://sherow1982.github.io/arabsad-ads/assets/images/logo.svg"

def determine_page_type(file_path: Path) -> str:
    relative = str(file_path.relative_to(Path("."))).lower()
    if 'blog/articles' in relative:
        return 'article'
    elif 'blog' in relative:
        return 'blog'
    elif 'services' in relative:
        return 'service'
    elif 'cities' in relative:
        return 'city'
    return 'page'

def build_page_url(file_path: Path) -> str:
    relative_path = file_path.relative_to(Path("."))
    url_path = str(relative_path).replace("\\", "/")
    return f"https://sherow1982.github.io/arabsad-ads/{url_path}"

def extract_page_keywords(file_path: Path, title: str) -> list:
    keywords = [
        "Google Ads", "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¬ÙˆØ¬Ù„", "Facebook Ads", "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª ÙÙŠØ³Ø¨ÙˆÙƒ",
        "SEO", "ØªØ­Ø³ÙŠÙ† Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¨Ø­Ø«", "ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ", "Ø§Ù„ØªØ³ÙˆÙŠÙ‚ Ø§Ù„Ø±Ù‚Ù…ÙŠ",
        "ØªØµÙ…ÙŠÙ… Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹", "Web Design", "Social Media Ads"
    ]
    
    for country_code, country_data in GULF_COUNTRIES.items():
        keywords.append(f"ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ {country_data['name']}")
        keywords.append(f"Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¬ÙˆØ¬Ù„ {country_data['name']}")
        for city_name, _, _ in country_data["cities"][:2]:
            keywords.append(f"ØªØ³ÙˆÙŠÙ‚ {city_name}")
    
    keywords.append(title)
    return list(set(keywords))[:25]

# ================== Schema ==================

def create_service_schema(title: str, image: str, url: str, description: str) -> str:
    import json
    area_served = [{"@type": "Country", "name": country_data['arabic_name']} for country_data in GULF_COUNTRIES.values()]
    
    schema = {
        "@context": "https://schema.org/",
        "@type": "Service",
        "name": title,
        "image": image,
        "description": description,
        "provider": {
            "@type": "Organization",
            "name": "Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨",
            "url": "https://sherow1982.github.io/arabsad-ads/",
            "logo": "https://sherow1982.github.io/arabsad-ads/assets/images/logo.svg",
            "telephone": "+201110760081"
        },
        "url": url,
        "areaServed": area_served,
        "priceRange": "$$-$$$"
    }
    return json.dumps(schema, ensure_ascii=False, indent=2)

def create_article_schema(title: str, image: str, url: str, description: str, file_path: Path) -> str:
    import json
    try:
        date_modified = datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
    except:
        date_modified = datetime.now().isoformat()
    
    schema = {
        "@context": "https://schema.org",
        "@type": "Article",
        "headline": title,
        "image": image,
        "description": description,
        "datePublished": date_modified,
        "dateModified": date_modified,
        "author": {"@type": "Organization", "name": "Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨"},
        "url": url
    }
    return json.dumps(schema, ensure_ascii=False, indent=2)

def create_organization_schema() -> str:
    import json
    schema = {
        "@context": "https://schema.org",
        "@type": "Organization",
        "name": "Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨",
        "url": "https://sherow1982.github.io/arabsad-ads/",
        "telephone": "+201110760081",
        "email": "info@arabsad.com"
    }
    return json.dumps(schema, ensure_ascii=False, indent=2)

def create_breadcrumb_schema(file_path: Path) -> str:
    import json
    relative = file_path.relative_to(Path("."))
    parts = relative.parts
    breadcrumb_items = [{"@type": "ListItem", "position": 1, "name": "Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©", "item": "https://sherow1982.github.io/arabsad-ads"}]
    
    current_path = ""
    for i, part in enumerate(parts[:-1], start=2):
        current_path += f"/{part}" if current_path else part
        name = part.replace('-', ' ').title()
        breadcrumb_items.append({
            "@type": "ListItem",
            "position": i,
            "name": name,
            "item": f"https://sherow1982.github.io/arabsad-ads/{current_path}"
        })
    
    schema = {"@context": "https://schema.org", "@type": "BreadcrumbList", "itemListElement": breadcrumb_items}
    return json.dumps(schema, ensure_ascii=False, indent=2)

def create_meta_tags(title: str, image: str, url: str, description: str, keywords: list) -> str:
    if len(description) > 155:
        desc_short = description[:152] + "..."
    else:
        desc_short = description
    
    title_clean = title.replace('"', '').replace("'", '')
    keywords_str = ", ".join(keywords[:15])
    
    meta = f"""
    <!-- SEO Meta Tags (Auto) -->
    <meta charset="UTF-8">
    <title>{title_clean} - Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨</title>
    <meta name="description" content="{desc_short}">
    <meta name="keywords" content="{keywords_str}">
    <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="canonical" href="{url}">
    <!-- Open Graph -->
    <meta property="og:title" content="{title_clean} - Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨">
    <meta property="og:description" content="{desc_short}">
    <meta property="og:image" content="{image}">
    <meta property="og:url" content="{url}">
    """
    return meta

# ================== Google Business Profiles ==================

def create_google_business_profiles():
    """Ø¥Ù†Ø´Ø§Ø¡ Google Business Profile Data Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¯Ù†"""
    profiles = []
    
    print("\nğŸ“Š Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Google Business Profiles:\n")
    
    for country_code, country_data in GULF_COUNTRIES.items():
        print(f"ğŸŒ {country_data['name']} ({country_code}):")
        for city_name, lat, lng in country_data["cities"]:
            profile = {
                "id": f"{country_code}_{city_name.replace(' ', '_')}",
                "business_name": f"Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ - {city_name}",
                "country_code": country_code,
                "country_name": country_data['arabic_name'],
                "city": city_name,
                "phone": "+201110760081",
                "website": "https://sherow1982.github.io/arabsad-ads/",
                "latitude": lat,
                "longitude": lng,
                "services": [
                    "Google Ads", "Facebook Ads", "Instagram Ads", "SEO",
                    "ØªØµÙ…ÙŠÙ… Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹", "Ø§Ù„ØªØ³ÙˆÙŠÙ‚ Ø§Ù„Ø±Ù‚Ù…ÙŠ", "ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…ØªØ§Ø¬Ø±"
                ],
                "address": f"{city_name}, {country_data['name']}",
                "opening_hours": {
                    "saturday": "08:00-23:00",
                    "sunday": "08:00-23:00",
                    "monday": "08:00-23:00",
                    "tuesday": "08:00-23:00",
                    "wednesday": "08:00-23:00",
                    "thursday": "08:00-23:00",
                    "friday": "08:00-23:00"
                }
            }
            profiles.append(profile)
            print(f"   âœ… {city_name}")
        print()
    
    return profiles

# ================== Sitemap ==================

def generate_sitemap(all_files: list) -> str:
    sitemap = '<?xml version="1.0" encoding="UTF-8"?>\n<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n'
    
    for file_path in all_files:
        if file_path.name.endswith('.html'):
            url = build_page_url(file_path)
            try:
                last_mod = datetime.fromtimestamp(file_path.stat().st_mtime).strftime('%Y-%m-%d')
            except:
                last_mod = datetime.now().strftime('%Y-%m-%d')
            
            priority = "1.0" if file_path.name == 'index.html' else "0.7"
            
            sitemap += f"""  <url>
    <loc>{url}</loc>
    <lastmod>{last_mod}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>{priority}</priority>
  </url>
"""
    
    sitemap += '</urlset>'
    return sitemap

# ================== Robots.txt ==================

def generate_robots_txt() -> str:
    return """User-agent: *
Allow: /
Disallow: /admin/

Sitemap: https://sherow1982.github.io/arabsad-ads/sitemap.xml
Crawl-delay: 1
"""

# ================== Ø§Ù„Ø­Ù‚Ù† ==================

def inject_seo(html: str, title: str, image: str, url: str, description: str, file_path: Path, page_type: str, keywords: list) -> str:
    if '</head>' not in html:
        if '<body' in html.lower():
            html = html.replace('<body', '</head><body', 1)
        else:
            html = html + '</head>'
    
    html = re.sub(r'<script\s+type=["\']?application/ld\+json["\']?\s*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
    
    meta = create_meta_tags(title, image, url, description, keywords)
    
    if page_type == 'article':
        main_schema = create_article_schema(title, image, url, description, file_path)
    else:
        main_schema = create_service_schema(title, image, url, description)
    
    org_schema = create_organization_schema()
    breadcrumb_schema = create_breadcrumb_schema(file_path)
    
    injection = f"""
{meta}

<!-- Main Schema (Auto) -->
<script type="application/ld+json">
{main_schema}
</script>

<!-- Organization Schema (Auto) -->
<script type="application/ld+json">
{org_schema}
</script>

<!-- Breadcrumb Schema (Auto) -->
<script type="application/ld+json">
{breadcrumb_schema}
</script>

</head>"""
    
    return html.replace('</head>', injection, 1)

def process_file(file_path: Path) -> tuple:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            html = f.read()
        
        title = extract_title(html)
        image = extract_image(html)
        description = extract_description(html)
        url = build_page_url(file_path)
        page_type = determine_page_type(file_path)
        keywords = extract_page_keywords(file_path, title)
        
        updated = inject_seo(html, title, image, url, description, file_path, page_type, keywords)
        
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(updated)
        
        return (True, file_path.relative_to(Path(".")))
    except Exception as e:
        return (False, file_path.relative_to(Path(".")))

def main():
    print("\n" + "="*80)
    print("ğŸ† Ø³ÙƒØ±Ø¨Øª SEO + Google Business Profiles ÙƒØ§Ù…Ù„Ø© - arabsad-ads ğŸ†")
    print("="*80 + "\n")

    root = Path(".")
    
    search_paths = [
        ("root", root, "*.html"),
        ("services", root / "services", "*.html"),
        ("cities", root / "cities", "*.html"),
        ("blog", root / "blog", "*.html"),
        ("articles", root / "blog" / "articles", "*.html"),
    ]
    
    all_files = []
    for folder_name, folder_path, pattern in search_paths:
        if folder_path.exists():
            files = sorted(folder_path.glob(pattern))
            all_files.extend(files)
            if files:
                print(f"ğŸ“‚ {folder_name}: {len(files)} Ù…Ù„Ù")
    
    if not all_files:
        print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª HTML")
        sys.exit(1)

    print(f"\nğŸ“¦ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {len(all_files)}\n")

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
    ok = 0
    for i, fp in enumerate(all_files, 1):
        print(f"[{i}/{len(all_files)}] {fp.relative_to(root)} ...", end=" ")
        success, _ = process_file(fp)
        if success:
            print("âœ…")
            ok += 1
        else:
            print("âŒ")

    # Ø¥Ù†Ø´Ø§Ø¡ Sitemap
    print("\nğŸ“ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Sitemap...")
    sitemap_content = generate_sitemap(all_files)
    with open(root / "sitemap.xml", "w", encoding="utf-8") as f:
        f.write(sitemap_content)
    print("âœ… sitemap.xml ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§\n")

    # Ø¥Ù†Ø´Ø§Ø¡ robots.txt
    print("ğŸ¤– Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ robots.txt...")
    with open(root / "robots.txt", "w", encoding="utf-8") as f:
        f.write(generate_robots_txt())
    print("âœ… robots.txt ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§\n")

    # Ø¥Ù†Ø´Ø§Ø¡ Google Business Profiles
    print("ğŸª Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Google Business Profiles...\n")
    profiles = create_google_business_profiles()
    
    # Ø­ÙØ¸ JSON Ø§Ù„ÙƒØ§Ù…Ù„
    json_data = json.dumps(profiles, ensure_ascii=False, indent=2)
    gbp_file = root / "google-business-profiles.json"
    with open(gbp_file, "w", encoding="utf-8") as f:
        f.write(json_data)
    
    print(f"âœ… google-business-profiles.json ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§\n")

    # Ø­ÙØ¸ Ù…Ù„ÙØ§Øª Ù…Ù†ÙØµÙ„Ø© Ù„ÙƒÙ„ Ø¯ÙˆÙ„Ø©
    print("ğŸ“ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„ÙØ§Øª Ù…Ù†ÙØµÙ„Ø© Ù„ÙƒÙ„ Ø¯ÙˆÙ„Ø©:\n")
    for country_code, country_data in GULF_COUNTRIES.items():
        country_profiles = [p for p in profiles if p['country_code'] == country_code]
        country_file = root / f"gbp-{country_code.lower()}.json"
        with open(country_file, "w", encoding="utf-8") as f:
            f.write(json.dumps(country_profiles, ensure_ascii=False, indent=2))
        print(f"   âœ… gbp-{country_code.lower()}.json ({len(country_profiles)} Ù…Ù„Ù)")

    # Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    print("\n" + "="*80)
    print("âœ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
    print("="*80)
    print(f"âœ… Ù…Ù„ÙØ§Øª HTML Ù…Ø­Ø¯Ø«Ø©: {ok}/{len(all_files)}")
    print(f"ğŸ“ Ù…Ù„ÙØ§Øª JSON:")
    print(f"   â€¢ google-business-profiles.json ({len(profiles)} Ù…Ù„Ù ØªØ¹Ø±ÙŠÙ)")
    for country_code in GULF_COUNTRIES.keys():
        print(f"   â€¢ gbp-{country_code.lower()}.json")
    
    print(f"\nğŸŒ Ø¯ÙˆÙ„ Ø§Ù„Ø®Ù„ÙŠØ¬:")
    total_cities = 0
    for code, country in GULF_COUNTRIES.items():
        city_count = len(country["cities"])
        total_cities += city_count
        print(f"   âœ… {country['name']}: {city_count} Ù…Ø¯ÙŠÙ†Ø©")
    
    print(f"\nğŸ’° Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {len(profiles)} Google Business Profile")
    print(f"ğŸ“ Ù…Ù† {len(GULF_COUNTRIES)} Ø¯ÙˆÙ„ Ùˆ {total_cities} Ù…Ø¯ÙŠÙ†Ø©")
    print("\n" + "="*80 + "\n")

if __name__ == "__main__":
    main()
