#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø³ÙƒØ±Ø¨Øª Ù…ØªÙƒØ§Ù…Ù„ Ù„Ù€ SEO ÙˆLocal Business Ùˆ Content Optimization
Ø±ÙŠØ¨Ùˆ: arabsad-ads (Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨)
Ø§Ù„Ù…Ù‡Ø§Ù…:
1. Ø­Ù‚Ù† Schema ÙˆMeta Tags
2. Ø¥Ù†Ø´Ø§Ø¡ Google Business Profile Data
3. Keyword Research Ù…Ø­Ø³Ù‘Ù†
4. Content Optimization
5. Internal Links
6. Sitemap XML
7. robots.txt
"""

import sys
import re
import json
from pathlib import Path
from datetime import datetime, timedelta
from urllib.parse import urljoin

# ================== Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ==================

GULF_COUNTRIES = {
    "SA": {
        "name": "Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©",
        "arabic_name": "Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©",
        "cities": ["Ø§Ù„Ø±ÙŠØ§Ø¶", "Ø¬Ø¯Ø©", "Ø§Ù„Ø¯Ù…Ø§Ù…"],
        "lat": 24.7136,
        "lng": 46.6753,
        "keywords": ["ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©", "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¬ÙˆØ¬Ù„ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©", "Google Ads Ø§Ù„Ø±ÙŠØ§Ø¶", "Facebook Ads Ø¬Ø¯Ø©"],
    },
    "AE": {
        "name": "Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª",
        "arabic_name": "Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø©",
        "cities": ["Ø¯Ø¨ÙŠ", "Ø£Ø¨ÙˆØ¸Ø¨ÙŠ", "Ø§Ù„Ø´Ø§Ø±Ù‚Ø©"],
        "lat": 23.4241,
        "lng": 53.8478,
        "keywords": ["ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª", "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¬ÙˆØ¬Ù„ Ø¯Ø¨ÙŠ", "Google Ads Ø£Ø¨ÙˆØ¸Ø¨ÙŠ", "SEO Ø¯Ø¨ÙŠ"],
    },
    "KW": {
        "name": "Ø§Ù„ÙƒÙˆÙŠØª",
        "arabic_name": "Ø¯ÙˆÙ„Ø© Ø§Ù„ÙƒÙˆÙŠØª",
        "cities": ["Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„ÙƒÙˆÙŠØª", "Ø§Ù„Ø£Ø­Ù…Ø¯ÙŠ"],
        "lat": 29.3759,
        "lng": 47.9774,
        "keywords": ["ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ø§Ù„ÙƒÙˆÙŠØª", "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¬ÙˆØ¬Ù„ Ø§Ù„ÙƒÙˆÙŠØª", "SEO Ø§Ù„ÙƒÙˆÙŠØª", "Facebook Ads Ø§Ù„ÙƒÙˆÙŠØª"],
    },
    "QA": {
        "name": "Ù‚Ø·Ø±",
        "arabic_name": "Ø¯ÙˆÙ„Ø© Ù‚Ø·Ø±",
        "cities": ["Ø§Ù„Ø¯ÙˆØ­Ø©"],
        "lat": 25.2854,
        "lng": 51.5310,
        "keywords": ["ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ù‚Ø·Ø±", "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¬ÙˆØ¬Ù„ Ù‚Ø·Ø±", "SEO Ø§Ù„Ø¯ÙˆØ­Ø©"],
    },
    "BH": {
        "name": "Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†",
        "arabic_name": "Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†",
        "cities": ["Ø§Ù„Ù…Ù†Ø§Ù…Ø©"],
        "lat": 26.0667,
        "lng": 50.5577,
        "keywords": ["ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†", "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¬ÙˆØ¬Ù„ Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†", "SEO Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†"],
    },
    "OM": {
        "name": "Ø¹Ù…Ø§Ù†",
        "arabic_name": "Ø³Ù„Ø·Ù†Ø© Ø¹Ù…Ø§Ù†",
        "cities": ["Ù…Ø³Ù‚Ø·"],
        "lat": 21.4735,
        "lng": 55.9754,
        "keywords": ["ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ø¹Ù…Ø§Ù†", "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¬ÙˆØ¬Ù„ Ø¹Ù…Ø§Ù†", "SEO Ù…Ø³Ù‚Ø·"],
    },
}

GLOBAL_KEYWORDS = {
    "google_ads": [
        "Google Ads",
        "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¬ÙˆØ¬Ù„",
        "Ø­Ù…Ù„Ø§Øª Ø¬ÙˆØ¬Ù„",
        "Google Search Ads",
        "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¨Ø­Ø«",
        "Google Display Network",
        "GDN",
    ],
    "facebook_ads": [
        "Facebook Ads",
        "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª ÙÙŠØ³Ø¨ÙˆÙƒ",
        "Instagram Ads",
        "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¥Ù†Ø³ØªØ¬Ø±Ø§Ù…",
        "Social Media Ads",
        "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„",
    ],
    "seo": [
        "SEO",
        "ØªØ­Ø³ÙŠÙ† Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¨Ø­Ø«",
        "Search Engine Optimization",
        "Ø§Ù„ØªØ±ØªÙŠØ¨ ÙÙŠ Ø¬ÙˆØ¬Ù„",
        "Ù…Ø­Ø³Ù† Ø§Ù„Ø¨Ø­Ø«",
        "Link Building",
        "Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·",
    ],
    "web_design": [
        "ØªØµÙ…ÙŠÙ… Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹",
        "Web Design",
        "ØªØµÙ…ÙŠÙ… Ù…ÙˆÙ‚Ø¹",
        "Website Design",
        "ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹",
        "Web Development",
    ],
}

# ================== Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ==================

def extract_title(html: str) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†"""
    m = re.search(r'<title[^>]*>([^<]+)</title>', html, re.IGNORECASE)
    if m:
        txt = m.group(1).strip()
        return txt.split('|')[0].strip() if '|' in txt else txt
    m = re.search(r'<h1[^>]*>([^<]+)</h1>', html, re.IGNORECASE)
    if m:
        return m.group(1).strip()
    return "ØµÙØ­Ø© Ù…Ù† Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨"

def extract_description(html: str) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØµÙ"""
    m = re.search(r'<meta\s+name=["\']description["\']\s+content=["\']([^"\']+)["\']', html, re.IGNORECASE)
    if m:
        return m.group(1).strip()
    m = re.search(r'<p[^>]*>([^<]+)</p>', html, re.IGNORECASE)
    if m:
        txt = m.group(1).strip()
        return txt if len(txt) <= 155 else txt[:152] + "..."
    return "Ø®Ø¯Ù…Ø§Øª ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ù…ØªÙ…ÙŠØ²Ø© Ù…Ù† Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨"

def extract_image(html: str) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±Ø©"""
    m = re.search(r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>', html, re.IGNORECASE)
    if m:
        src = m.group(1).strip()
        if src.startswith('http'):
            return src
        src = src.lstrip('./')
        return f"https://sherow1982.github.io/arabsad-ads/{src}"
    return "https://sherow1982.github.io/arabsad-ads/assets/images/logo.svg"

def determine_page_type(file_path: Path) -> str:
    """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØµÙØ­Ø©"""
    relative = str(file_path.relative_to(Path("."))).lower()
    if 'blog/articles' in relative:
        return 'article'
    elif 'blog' in relative:
        return 'blog'
    elif 'services' in relative:
        return 'service'
    elif 'cities' in relative:
        return 'city'
    return 'page'

def build_page_url(file_path: Path) -> str:
    """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±Ø§Ø¨Ø·"""
    relative_path = file_path.relative_to(Path("."))
    url_path = str(relative_path).replace("\\", "/")
    return f"https://sherow1982.github.io/arabsad-ads/{url_path}"

def extract_page_keywords(file_path: Path, title: str) -> list:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ keywords Ù…Ù† Ø§Ù„ØµÙØ­Ø© ÙˆØ¥Ø¶Ø§ÙØ© keywords Ø§Ù„Ø®Ù„ÙŠØ¬"""
    keywords = []
    
    # Ø£Ø¶Ù keywords Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø®Ø¯Ù…Ø©
    if 'google-ads' in str(file_path).lower():
        keywords.extend(GLOBAL_KEYWORDS["google_ads"])
    elif 'seo' in str(file_path).lower():
        keywords.extend(GLOBAL_KEYWORDS["seo"])
    elif 'social' in str(file_path).lower():
        keywords.extend(GLOBAL_KEYWORDS["facebook_ads"])
    elif 'website' in str(file_path).lower() or 'design' in str(file_path).lower():
        keywords.extend(GLOBAL_KEYWORDS["web_design"])
    
    # Ø£Ø¶Ù keywords Ø®Ù„ÙŠØ¬ Ø¹Ø§Ù…
    for country_data in GULF_COUNTRIES.values():
        keywords.extend(country_data["keywords"][:2])
    
    # Ø£Ø¶Ù Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
    keywords.append(title)
    
    return list(set(keywords))[:15]

# ================== Schema ÙˆMeta ==================

def create_service_schema(title: str, image: str, url: str, description: str) -> str:
    """Service Schema"""
    import json
    schema = {
        "@context": "https://schema.org/",
        "@type": "Service",
        "name": title,
        "image": image,
        "description": description,
        "provider": {
            "@type": "Organization",
            "name": "Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨",
            "url": "https://sherow1982.github.io/arabsad-ads/",
            "logo": "https://sherow1982.github.io/arabsad-ads/assets/images/logo.svg",
            "telephone": "+201110760081"
        },
        "url": url,
        "areaServed": [
            {"@type": "Country", "name": "Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©"},
            {"@type": "Country", "name": "Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª"},
            {"@type": "Country", "name": "Ø§Ù„ÙƒÙˆÙŠØª"},
            {"@type": "Country", "name": "Ù‚Ø·Ø±"},
            {"@type": "Country", "name": "Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†"},
            {"@type": "Country", "name": "Ø¹Ù…Ø§Ù†"},
        ],
        "priceRange": "$$-$$$",
        "potentialAction": {
            "@type": "ReserveAction",
            "target": {
                "@type": "EntryPoint",
                "urlTemplate": "https://wa.me/201110760081?text=Ø£Ø±ÙŠØ¯ Ø§Ø³ØªØ´Ø§Ø±Ø©"
            }
        }
    }
    return json.dumps(schema, ensure_ascii=False, indent=2)

def create_article_schema(title: str, image: str, url: str, description: str, file_path: Path) -> str:
    """Article Schema"""
    import json
    try:
        date_modified = datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
    except:
        date_modified = datetime.now().isoformat()
    
    schema = {
        "@context": "https://schema.org",
        "@type": "Article",
        "headline": title,
        "image": image,
        "description": description,
        "datePublished": date_modified,
        "dateModified": date_modified,
        "author": {
            "@type": "Organization",
            "name": "Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨"
        },
        "publisher": {
            "@type": "Organization",
            "name": "Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨",
            "logo": {
                "@type": "ImageObject",
                "url": "https://sherow1982.github.io/arabsad-ads/assets/images/logo.svg"
            }
        },
        "url": url
    }
    return json.dumps(schema, ensure_ascii=False, indent=2)

def create_organization_schema() -> str:
    """Organization Schema Ù…Ø¹ Local Business"""
    import json
    schema = {
        "@context": "https://schema.org",
        "@type": "Organization",
        "name": "Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨",
        "alternateName": "ArabSad Digital Marketing",
        "image": "https://sherow1982.github.io/arabsad-ads/assets/images/logo.svg",
        "description": "ÙˆÙƒØ§Ù„Ø© ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ù…ØªØ®ØµØµØ© ÙÙŠ Google Ads ÙˆFacebook Ads ÙˆSEO ÙˆØªØµÙ…ÙŠÙ… Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹",
        "url": "https://sherow1982.github.io/arabsad-ads/",
        "telephone": "+201110760081",
        "email": "info@arabsad.com",
        "address": {
            "@type": "PostalAddress",
            "addressCountry": "EG",
            "addressRegion": "Ø§Ù„Ø¬ÙŠØ²Ø©",
            "addressLocality": "Ø­Ø¯Ø§Ø¦Ù‚ Ø£ÙƒØªÙˆØ¨Ø±"
        },
        "sameAs": [
            "https://www.facebook.com/arabsad",
            "https://www.twitter.com/arabsad",
            "https://www.instagram.com/arabsad"
        ]
    }
    return json.dumps(schema, ensure_ascii=False, indent=2)

def create_local_business_schemas() -> dict:
    """LocalBusiness Schema Ù„ÙƒÙ„ Ø¯ÙˆÙ„ Ø§Ù„Ø®Ù„ÙŠØ¬"""
    import json
    schemas = {}
    
    for country_code, country_data in GULF_COUNTRIES.items():
        schema = {
            "@context": "https://schema.org",
            "@type": "LocalBusiness",
            "name": f"Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ - {country_data['name']}",
            "image": "https://sherow1982.github.io/arabsad-ads/assets/images/logo.svg",
            "url": "https://sherow1982.github.io/arabsad-ads/",
            "telephone": "+201110760081",
            "address": {
                "@type": "PostalAddress",
                "addressCountry": country_code,
                "addressLocality": country_data['cities'][0]
            },
            "geo": {
                "@type": "GeoCoordinates",
                "latitude": country_data['lat'],
                "longitude": country_data['lng']
            },
            "openingHours": "Su-Sa 08:00-23:00",
            "priceRange": "$$",
            "areaServed": country_data['name']
        }
        schemas[country_code] = json.dumps(schema, ensure_ascii=False, indent=2)
    
    return schemas

def create_breadcrumb_schema(file_path: Path) -> str:
    """Breadcrumb Schema"""
    import json
    relative = file_path.relative_to(Path("."))
    parts = relative.parts
    breadcrumb_items = []
    base_url = "https://sherow1982.github.io/arabsad-ads"
    
    breadcrumb_items.append({
        "@type": "ListItem",
        "position": 1,
        "name": "Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©",
        "item": base_url
    })
    
    current_path = ""
    for i, part in enumerate(parts[:-1], start=2):
        current_path += f"/{part}" if current_path else part
        name = part.replace('-', ' ').title()
        breadcrumb_items.append({
            "@type": "ListItem",
            "position": i,
            "name": name,
            "item": f"{base_url}/{current_path}"
        })
    
    schema = {
        "@context": "https://schema.org",
        "@type": "BreadcrumbList",
        "itemListElement": breadcrumb_items
    }
    return json.dumps(schema, ensure_ascii=False, indent=2)

def create_meta_tags(title: str, image: str, url: str, description: str, keywords: list) -> str:
    """Meta Tags Ù…Ø­Ø³Ù‘Ù†Ø© Ù…Ø¹ Keywords"""
    if len(description) > 155:
        desc_short = description[:152] + "..."
    else:
        desc_short = description
    
    title_clean = title.replace('"', '').replace("'", '')
    keywords_str = ", ".join(keywords[:10])
    
    meta = f"""
    <!-- SEO Meta Tags (Auto) -->
    <meta charset="UTF-8">
    <title>{title_clean} - Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ | ÙˆÙƒØ§Ù„Ø© ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ø§Ù„Ø®Ù„ÙŠØ¬</title>
    <meta name="description" content="{desc_short}">
    <meta name="keywords" content="{keywords_str}">
    <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="language" content="ar">
    <meta name="author" content="Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨">
    <meta name="geo.region" content="EG">
    <meta name="geo.placename" content="Ù…ØµØ±">
    <link rel="canonical" href="{url}">
    <link rel="alternate" hreflang="ar" href="{url}">
    <!-- Open Graph -->
    <meta property="og:title" content="{title_clean} - Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨">
    <meta property="og:description" content="{desc_short}">
    <meta property="og:image" content="{image}">
    <meta property="og:url" content="{url}">
    <meta property="og:type" content="website">
    <meta property="og:site_name" content="Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨">
    <meta property="og:locale" content="ar_EG">
    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="{title_clean}">
    <meta name="twitter:description" content="{desc_short}">
    <meta name="twitter:image" content="{image}">
    """
    return meta

# ================== Local Business Data ==================

def create_google_business_profile_json() -> str:
    """Ø¥Ù†Ø´Ø§Ø¡ Google Business Profile Data JSON"""
    import json
    profiles = []
    
    for country_code, country_data in GULF_COUNTRIES.items():
        for city in country_data['cities']:
            profile = {
                "business_name": f"Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ - {city}",
                "country_code": country_code,
                "country_name": country_data['arabic_name'],
                "city": city,
                "phone": "+201110760081",
                "website": "https://sherow1982.github.io/arabsad-ads/",
                "latitude": country_data['lat'],
                "longitude": country_data['lng'],
                "services": [
                    "Google Ads",
                    "Facebook Ads",
                    "SEO",
                    "ØªØµÙ…ÙŠÙ… Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹",
                    "Ø§Ù„ØªØ³ÙˆÙŠÙ‚ Ø§Ù„Ø±Ù‚Ù…ÙŠ"
                ],
                "opening_hours": {
                    "monday": "08:00-23:00",
                    "tuesday": "08:00-23:00",
                    "wednesday": "08:00-23:00",
                    "thursday": "08:00-23:00",
                    "friday": "08:00-23:00",
                    "saturday": "08:00-23:00",
                    "sunday": "08:00-23:00"
                },
                "service_areas": [city, country_data['name']],
                "keywords": country_data['keywords']
            }
            profiles.append(profile)
    
    return json.dumps(profiles, ensure_ascii=False, indent=2)

# ================== Internal Links ==================

def generate_internal_links(file_path: Path, all_files: list) -> list:
    """ØªÙˆÙ„ÙŠØ¯ internal links Ø°ÙƒÙŠØ©"""
    page_type = determine_page_type(file_path)
    links = []
    
    # Ø£Ø¶Ù Ø±Ø§Ø¨Ø· Ù„Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    links.append({
        "url": "https://sherow1982.github.io/arabsad-ads/index.html",
        "text": "Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©",
        "anchor_text": '<a href="/">Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©</a>'
    })
    
    # Ø£Ø¶Ù Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù†ÙØ³ Ø§Ù„ÙØ¦Ø©
    current_folder = file_path.parent
    related_files = [f for f in all_files if f.parent == current_folder and f != file_path]
    
    for related in related_files[:3]:
        title = extract_title(open(related, 'r', encoding='utf-8').read())
        url = build_page_url(related)
        links.append({
            "url": url,
            "text": title,
            "anchor_text": f'<a href="{related.name}">{title}</a>'
        })
    
    # Ø£Ø¶Ù Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰
    services_dir = Path(".") / "services"
    if services_dir.exists() and page_type != 'service':
        for service_file in list(services_dir.glob("*.html"))[:3]:
            title = extract_title(open(service_file, 'r', encoding='utf-8').read())
            url = build_page_url(service_file)
            links.append({
                "url": url,
                "text": title,
                "anchor_text": f'<a href="services/{service_file.name}">{title}</a>'
            })
    
    return links[:5]

# ================== Content Optimization ==================

def generate_content_optimization_data(title: str, keywords: list, file_path: Path) -> dict:
    """ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
    return {
        "title": title,
        "keywords": keywords,
        "keyword_density": {
            keyword: f"Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ÙƒÙ„Ù…Ø© {keyword} 2-3 Ù…Ø±Ø§Øª ÙÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"
            for keyword in keywords[:5]
        },
        "content_guidelines": {
            "minimum_length": "1500+ ÙƒÙ„Ù…Ø©",
            "headings": "Ø§Ø³ØªØ®Ø¯Ù… H1ØŒ H2ØŒ H3 Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø¸Ù…",
            "images": "Ø£Ø¶Ù 3-5 ØµÙˆØ± Ù…Ø­Ø³Ù‘Ù†Ø©",
            "internal_links": "Ø£Ø¶Ù 5-10 Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠØ©",
            "external_links": "Ø£Ø¶Ù 2-3 Ø±ÙˆØ§Ø¨Ø· Ø®Ø§Ø±Ø¬ÙŠØ© Ù…ÙˆØ«ÙˆÙ‚Ø©",
            "readability": "Ø§Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø±Ø§Øª Ù‚ØµÙŠØ±Ø© ÙˆÙ‚ÙˆØ§Ø¦Ù…"
        },
        "seo_checklist": [
            "âœ… Meta Description (150-160 Ø­Ø±Ù)",
            "âœ… Keywords ÙÙŠ Ø§Ù„Ù€ Title",
            "âœ… Keywords ÙÙŠ Ø§Ù„Ù€ H1",
            "âœ… Keywords ÙÙŠ Ø£ÙˆÙ„ 100 ÙƒÙ„Ù…Ø©",
            "âœ… ØµÙˆØ± Ù…Ø¹ Alt Text",
            "âœ… Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠØ©",
            "âœ… Call to Action ÙˆØ§Ø¶Ø­"
        ]
    }

# ================== Sitemap ==================

def generate_sitemap(all_files: list) -> str:
    """ØªÙˆÙ„ÙŠØ¯ Sitemap XML"""
    sitemap = '<?xml version="1.0" encoding="UTF-8"?>\n'
    sitemap += '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n'
    
    base_url = "https://sherow1982.github.io/arabsad-ads"
    
    for file_path in all_files:
        if file_path.name.endswith('.html'):
            url = build_page_url(file_path)
            try:
                last_mod = datetime.fromtimestamp(file_path.stat().st_mtime).strftime('%Y-%m-%d')
            except:
                last_mod = datetime.now().strftime('%Y-%m-%d')
            
            # Ø­Ø¯Ø¯ Ø§Ù„Ø£Ù‡Ù…ÙŠØ© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„ØµÙØ­Ø©
            if file_path.name == 'index.html':
                priority = "1.0"
                changefreq = "daily"
            elif 'services' in str(file_path):
                priority = "0.8"
                changefreq = "weekly"
            elif 'blog' in str(file_path):
                priority = "0.7"
                changefreq = "weekly"
            else:
                priority = "0.6"
                changefreq = "monthly"
            
            sitemap += f"""  <url>
    <loc>{url}</loc>
    <lastmod>{last_mod}</lastmod>
    <changefreq>{changefreq}</changefreq>
    <priority>{priority}</priority>
  </url>
"""
    
    sitemap += '</urlset>'
    return sitemap

# ================== Robots.txt ==================

def generate_robots_txt() -> str:
    """ØªÙˆÙ„ÙŠØ¯ robots.txt Ù…Ø­Ø³Ù‘Ù†"""
    robots = """User-agent: *
Allow: /
Disallow: /admin/
Disallow: /private/
Disallow: /?*
Disallow: /*?*
Disallow: /*.js$
Disallow: /*.css$

# Google Bot
User-agent: Googlebot
Allow: /

# Bing Bot
User-agent: Bingbot
Allow: /

# Sitemap
Sitemap: https://sherow1982.github.io/arabsad-ads/sitemap.xml

# Crawl Delay
Crawl-delay: 1
"""
    return robots

# ================== Ø§Ù„Ø­Ù‚Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ==================

def inject_seo(html: str, title: str, image: str, url: str, description: str, file_path: Path, page_type: str, keywords: list, local_business_schemas: dict) -> str:
    """Ø­Ù‚Ù† ÙƒÙ„ Ø´ÙŠØ¡ ÙÙŠ <head>"""
    if '</head>' not in html:
        if '<body' in html.lower():
            html = html.replace('<body', '</head><body', 1)
        else:
            html = html + '</head>'
    
    # Ø­Ø°Ù Schema Ø§Ù„Ù‚Ø¯ÙŠÙ…
    html = re.sub(
        r'<script\s+type=["\']?application/ld\+json["\']?\s*>.*?</script>',
        '',
        html,
        flags=re.DOTALL | re.IGNORECASE
    )
    
    meta = create_meta_tags(title, image, url, description, keywords)
    
    if page_type == 'article':
        main_schema = create_article_schema(title, image, url, description, file_path)
    else:
        main_schema = create_service_schema(title, image, url, description)
    
    org_schema = create_organization_schema()
    breadcrumb_schema = create_breadcrumb_schema(file_path)
    
    injection = f"""
{meta}

<!-- Service/Article Schema JSON-LD (Auto) -->
<script type="application/ld+json">
{main_schema}
</script>

<!-- Organization Schema JSON-LD (Auto) -->
<script type="application/ld+json">
{org_schema}
</script>

<!-- LocalBusiness Schema - Gulf Countries (Auto) -->
<script type="application/ld+json">
[
{', '.join(list(local_business_schemas.values())[:2])}
]
</script>

<!-- Breadcrumb Schema JSON-LD (Auto) -->
<script type="application/ld+json">
{breadcrumb_schema}
</script>

</head>"""
    
    return html.replace('</head>', injection, 1)

def process_file(file_path: Path, all_files: list, local_business_schemas: dict) -> tuple:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù ÙˆØ§Ø­Ø¯"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            html = f.read()
        
        title = extract_title(html)
        image = extract_image(html)
        description = extract_description(html)
        url = build_page_url(file_path)
        page_type = determine_page_type(file_path)
        keywords = extract_page_keywords(file_path, title)
        
        updated = inject_seo(html, title, image, url, description, file_path, page_type, keywords, local_business_schemas)
        
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(updated)
        
        return (True, file_path.relative_to(Path(".")), page_type, keywords)
    except Exception as e:
        return (False, file_path.relative_to(Path(".")), str(e), [])

def main():
    print("\n" + "="*80)
    print("ğŸ† Ø³ÙƒØ±Ø¨Øª SEO Ø´Ø§Ù…Ù„ + Local Business + Content Optimization - arabsad-ads ğŸ†")
    print("="*80 + "\n")

    root = Path(".")
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª HTML
    search_paths = [
        ("root", root, "*.html"),
        ("services", root / "services", "*.html"),
        ("cities", root / "cities", "*.html"),
        ("blog", root / "blog", "*.html"),
        ("articles", root / "blog" / "articles", "*.html"),
    ]
    
    all_files = []
    for folder_name, folder_path, pattern in search_paths:
        if folder_path.exists():
            files = sorted(folder_path.glob(pattern))
            all_files.extend(files)
            if files:
                print(f"ğŸ“‚ {folder_name}: {len(files)} Ù…Ù„Ù")
    
    if not all_files:
        print("\nâŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ù„ÙØ§Øª HTML")
        sys.exit(1)

    print(f"\nğŸ“¦ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {len(all_files)}\n")
    print("-" * 80 + "\n")

    # Ø¥Ù†Ø´Ø§Ø¡ Local Business Schemas
    local_business_schemas = create_local_business_schemas()
    
    ok = 0
    fail = 0
    stats = {"service": 0, "article": 0, "city": 0, "blog": 0, "page": 0}
    all_keywords = []

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
    for i, fp in enumerate(all_files, 1):
        rel_path = fp.relative_to(root)
        print(f"[{i}/{len(all_files)}] {rel_path} ...", end=" ")
        
        success, filename, result, keywords = process_file(fp, all_files, local_business_schemas)
        if success:
            page_type = result
            stats[page_type] = stats.get(page_type, 0) + 1
            all_keywords.extend(keywords)
            print(f"âœ… ({page_type})")
            ok += 1
        else:
            print(f"âŒ {result}")
            fail += 1

    # Ø¥Ù†Ø´Ø§Ø¡ Sitemap
    print("\nğŸ“ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Sitemap XML...")
    sitemap_content = generate_sitemap(all_files)
    with open(root / "sitemap.xml", "w", encoding="utf-8") as f:
        f.write(sitemap_content)
    print("   âœ… sitemap.xml ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§")

    # Ø¥Ù†Ø´Ø§Ø¡ robots.txt
    print("ğŸ¤– Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ robots.txt...")
    robots_content = generate_robots_txt()
    with open(root / "robots.txt", "w", encoding="utf-8") as f:
        f.write(robots_content)
    print("   âœ… robots.txt ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§")

    # Ø¥Ù†Ø´Ø§Ø¡ Google Business Profile JSON
    print("ğŸª Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Google Business Profile Data...")
    gbp_content = create_google_business_profile_json()
    with open(root / "google-business-profiles.json", "w", encoding="utf-8") as f:
        f.write(gbp_content)
    print("   âœ… google-business-profiles.json ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§")

    # Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    print("\n" + "="*80)
    print("ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
    print("="*80)
    print(f"âœ… Ù…Ù„ÙØ§Øª Ù…Ø­Ø¯Ø«Ø©: {ok}")
    print(f"âŒ Ù…Ù„ÙØ§Øª ÙØ´Ù„Øª: {fail}")
    print(f"ğŸ“ˆ Ù†Ø³Ø¨Ø© Ø§Ù„Ù†Ø¬Ø§Ø­: {(ok/len(all_files)*100):.1f}%")
    print("\nğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹:")
    for page_type, count in stats.items():
        if count > 0:
            print(f"   â€¢ {page_type}: {count} Ù…Ù„Ù")
    print("\nğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙÙ†Ø´Ø£Ø©:")
    print("   âœ… sitemap.xml - Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹ XML")
    print("   âœ… robots.txt - ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¨Ø­Ø«")
    print("   âœ… google-business-profiles.json - Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¯ÙˆÙ„ Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠØ©")
    print("\nğŸŒ Ø¯ÙˆÙ„ Ø§Ù„Ø®Ù„ÙŠØ¬ Ø§Ù„Ù…ÙØ¯Ø¹ÙˆÙ…Ø©:")
    for code, country in GULF_COUNTRIES.items():
        print(f"   â€¢ {country['name']} ({code})")

    print("\n" + "="*80)
    print("âœ¨ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø­Ù„ SEO Ù…ØªÙƒØ§Ù…Ù„!")
    print("="*80)
    print("\nğŸ“ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:")
    print("1. Ø§Ø±ÙØ¹ sitemap.xml Ùˆ robots.txt Ø¹Ù„Ù‰ GitHub")
    print("2. Ø§Ø¯Ø®Ù„ Google Search Console ÙˆØ£Ø¶Ù sitemap.xml")
    print("3. Ø§Ø³ØªØ®Ø¯Ù… Ø¨ÙŠØ§Ù†Ø§Øª google-business-profiles.json Ù„Ø¥Ù†Ø´Ø§Ø¡ GBP")
    print("4. Ø­Ø³Ù‘Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ù€ 1500+ ÙƒÙ„Ù…Ø© Ù…Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… Keywords")
    print("5. Ø£Ø¶Ù Internal Links Ø¨ÙŠÙ† Ø§Ù„ØµÙØ­Ø§Øª")
    print("\n")

if __name__ == "__main__":
    main()
